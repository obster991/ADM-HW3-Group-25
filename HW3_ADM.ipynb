{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b61cb8",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e300f25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import string\n",
    "import json\n",
    "\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english')\n",
    "\n",
    "# Append some recurrent words in synopsis:\n",
    "frequent_words = ['character','characters','end']\n",
    "default_stopwords.extend(frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28ef2f-ea36-4609-a2b9-36d83ad0b289",
   "metadata": {},
   "source": [
    "#### List of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713c6d86-b4f4-4aae-a531-847a7b5fa464",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_list_anime = \"C:/Users/andre/Desktop/list_anime.txt\"\n",
    "url_of_single_page = \"https://myanimelist.net/topanime.php?limit=\"\n",
    "path_page_general = \"C:/Users/andre/Desktop/Anime/Page\"\n",
    "path_documents_file = \"C:/Users/andre/Desktop/Anime/documents.tsv\"\n",
    "path_vocabulary_file = \"C:/Users/andre/Desktop/Anime/vocabulary.json\"\n",
    "path_index_file = \"C:/Users/andre/Desktop/Anime/index.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abc382-bcdd-4ab5-a315-768b8558a616",
   "metadata": {},
   "source": [
    "#### Write file with all anime urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d3ffca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 19130 anime in total, 383 pages with 50 animes\n",
    "\n",
    "def write_list_anime(path_list_anime, url_of_single_page):\n",
    "    # Number of total pages\n",
    "    n_pages = 383 \n",
    "\n",
    "    with open(path_list_anime, \"w\", encoding='utf-8') as file:\n",
    "\n",
    "        for page in tqdm(range(0, n_pages)):\n",
    "\n",
    "            # URL of the single page \n",
    "            url = url_of_single_page + str(page * 50)\n",
    "\n",
    "            # Get response object\n",
    "            response = r.get(url)\n",
    "\n",
    "            # Get html\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all links iterating through the document\n",
    "            for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "                a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "                for a in a_list:\n",
    "                    link = a['href']\n",
    "                    file.write(str(link) + '\\n')\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce53ce4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def download_html(path_list_anime, path_page_general):\n",
    "    file = open(path_list_anime, \"r\", encoding='utf-8')\n",
    "\n",
    "    # Initialize counters \n",
    "    counter_anime = 0\n",
    "    page = 0\n",
    "\n",
    "    # Get html file for every anime (url is a line in the file list_anime) \n",
    "    for line in tqdm(file):\n",
    "        counter_anime += 1\n",
    "        if (counter_anime%50 == 1):\n",
    "            page +=1\n",
    "        response = r.get(line)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            os.mkdir(path_page_general + str(page))\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        with open(path_page_general + str(page) + \"/article_\" + str(counter_anime) + \".html\", \"w\", encoding='utf-8') as file:\n",
    "            file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438979ef",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bd65c-48df-4bae-acd2-1e6e52af4476",
   "metadata": {},
   "source": [
    "#### Function to retrieve all the information from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9981071",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Retrive information at page i of anime j\n",
    "def retrive_information(i, j):\n",
    "        with open(path_page_general + str(i) + \"/anime\" + str(j) + \".html\", \"r\", encoding='utf-8') as file:\n",
    "    \n",
    "            # Read file and parse html code\n",
    "            content_page = file.read()\n",
    "            soup = BeautifulSoup(content_page, 'html.parser')\n",
    "            \n",
    "            # Retrive information \n",
    "            animeTitle = soup.find_all(\"h1\", class_=\"title-name h1_bold_none\")[0].find(\"strong\").text\n",
    "        \n",
    "            animeType = soup.find_all('a', href = re.compile(r'type*'))[0].text.strip()\n",
    "            \n",
    "            animeNumEpisode = soup.find(string='Episodes:').next_element.replace(\"\\n\",\"\").strip()\n",
    "            \n",
    "            animeNumMembers = soup.find(string='Members:').next_element.replace(\"\\n\",\"\").replace(',','').replace('#', '').strip()\n",
    "        \n",
    "            animeScore = soup.find('span', {\"itemprop\":\"ratingValue\"}).text.strip()\n",
    "            \n",
    "            animeUsers = soup.find('span', {\"itemprop\":\"ratingCount\"}).text.strip()\n",
    "            \n",
    "            animeRank = soup.find(string='Ranked:').next_element.replace(\"\\n\",\"\").replace('#','').strip()\n",
    "            \n",
    "            animePopularity = soup.find(string='Popularity:').next_element.replace(\"\\n\",\"\").replace('#', '').strip()\n",
    "            \n",
    "            animeDescription = soup.find('p',{'itemprop':'description'}).text.replace('\\n', '').replace('  ', '').strip()\n",
    "\n",
    "    \n",
    "            # Iterate in di with dates   \n",
    "            dates_div = soup.find_all(\"div\", class_=\"spaceit_pad\")\n",
    "            for i in range(0, len(dates_div)):\n",
    "        \n",
    "                # Dates\n",
    "                if dates_div[i].find(\"span\", string='Aired:') != None:\n",
    "                    dates = dates_div[i].contents[2].strip().split(\"to\")\n",
    "                    try:\n",
    "                        releaseDate = datetime.strptime(dates[0].strip(), '%b %d, %Y').date()\n",
    "                    except:\n",
    "                        releaseDate = None\n",
    "                    try:\n",
    "                        endDate = datetime.strptime(dates[1].strip(), '%b %d, %Y').date()\n",
    "                    except:  \n",
    "                        endDate = None\n",
    "    \n",
    "            # Anime Description\n",
    "            animeDescription = soup.find(\"p\", itemprop='description').text\n",
    "    \n",
    "            # Related Animes\n",
    "            try:\n",
    "                related_anime_table = soup.find(\"table\", class_=\"anime_detail_related_anime\")\n",
    "                links_list = [link.text for link in related_anime_table.find_all(\"a\")]\n",
    "                animeRelated = list(set(links_list))\n",
    "            except:\n",
    "                animeRelated = None\n",
    "    \n",
    "            # Characters\n",
    "            animeCharacters = soup.find_all(\"h3\", class_=\"h3_characters_voice_actors\")\n",
    "            animeCharacters = [character.text for character in animeCharacters]\n",
    "    \n",
    "            # Voices\n",
    "            animeVoices = soup.find_all(\"td\", class_=\"va-t ar pl4 pr4\")\n",
    "            animeVoices = [actor.find(\"a\").text for actor in animeVoices]\n",
    "    \n",
    "            a_no = []\n",
    "            total = []\n",
    "            result = []\n",
    "            animeStaff = []\n",
    "    \n",
    "            # Staff \n",
    "            if animeCharacters == [] and animeVoices == []:\n",
    "                div = soup.find_all(\"div\", class_=\"detail-characters-list clearfix\")[0]\n",
    "                tds = div.find_all(\"td\", class_=lambda x: x != 'ac borderClass' and x != 'ac' and x == 'borderClass')\n",
    "    \n",
    "                smalls = div.find_all(\"small\")\n",
    "    \n",
    "                for td in tds:\n",
    "                    a_no = set(list(td.find_all(\"a\", class_=\"fw-n\")))\n",
    "                    total = set(list(td.find_all(\"a\")))\n",
    "                    if list(total.difference(a_no)) != []:\n",
    "                        result.append(list(total.difference(a_no)))\n",
    "    \n",
    "                for (s, role) in zip(result, smalls):\n",
    "                    s = str(s).strip(\"[\").strip(\"]\")\n",
    "                    htmlTag = BeautifulSoup(s, \"html.parser\")\n",
    "                    animeStaff.append([htmlTag.text, role.text])\n",
    "                \n",
    "            # There are no staff\n",
    "            elif len(soup.find_all(\"div\", class_=\"detail-characters-list clearfix\")) < 2:\n",
    "                animeStaff = []\n",
    "                \n",
    "            # There are one of animeCharacters or animeVoices\n",
    "            else:\n",
    "                div = soup.find_all(\"div\", class_=\"detail-characters-list clearfix\")[1]\n",
    "                \n",
    "                tds = div.find_all(\"td\", class_=lambda x: x != 'ac borderClass' and x != 'ac' and x == 'borderClass')\n",
    "\n",
    "                smalls = div.find_all(\"small\")\n",
    "\n",
    "                for td in tds:\n",
    "                    a_no = set(list(td.find_all(\"a\", class_=\"fw-n\")))\n",
    "                    total = set(list(td.find_all(\"a\")))\n",
    "                    if list(total.difference(a_no)) != []:\n",
    "                        result.append(list(total.difference(a_no)))\n",
    "\n",
    "                for (s, role) in zip(result, smalls):\n",
    "                    s = str(s).strip(\"[\").strip(\"]\")\n",
    "                    htmlTag = BeautifulSoup(s, \"html.parser\")\n",
    "                    animeStaff.append([htmlTag.text, role.text])\n",
    "            \n",
    "        # Create list with overall information\n",
    "        final_information = [animeTitle, animeType,animeNumEpisode,releaseDate,endDate,\n",
    "                         animeNumMembers, animeScore,animeUsers,animeRank,animePopularity,\n",
    "                         animeDescription,animeRelated,animeCharacters,animeVoices,animeStaff]\n",
    "        return final_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951208c-42e9-4324-802c-465639f79315",
   "metadata": {},
   "source": [
    "#### Create a .tsv file for each anime and a .tsv file with 3 most important information for all: Title, Description, Url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca73b577",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_tsvs(path_list_anime, path_page_general, path_documents_file):\n",
    "    # Create .tsv file\n",
    "    n_pages = 384\n",
    "\n",
    "    # Collecting all Urls\n",
    "    with open(path_list_anime, 'r', encoding='utf-8') as f_urls:\n",
    "        urls = f_urls.readlines()\n",
    "    f_urls.close\n",
    "\n",
    "    # Create .tsv with only 3 important fields\n",
    "    with open(path_documents_file, 'w', encoding='utf-8') as f:\n",
    "        tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "        # Write haeders\n",
    "        tsv_writer.writerow(['animeTitle','animeDescription','Url'])\n",
    "\n",
    "    f.close\n",
    "\n",
    "    # Iterating through pages and then animes\n",
    "    for i in range(1, n_pages):\n",
    "\n",
    "        # Start and end computed with respect to the \n",
    "        start_iterate_anime = (i-1)*50+1\n",
    "        end_iterate_anime = ((i-1)*50+1)+50\n",
    "        for j in range(start_iterate_anime, end_iterate_anime):\n",
    "\n",
    "            # Write for each anime a .tsv file\n",
    "            with open(path_page_general + str(i) + \"/anime_\" + str(j)+ \".tsv\", 'w', encoding='utf-8') as out_file:\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "                # Write haeders\n",
    "                tsv_writer.writerow(['animeTitle', 'animeType','animeNumEpisode','releaseDate','endDate',\n",
    "                                     'animeNumMembers', 'animeScore','animeUsers','animeRank','animePopularity',\n",
    "                                     'animeDescription','animeRelated','animeCharacters','animeVoices','animeStaff'])\n",
    "\n",
    "                # Write information\n",
    "                information = retrive_information(i, j) \n",
    "                tsv_writer.writerow(information)\n",
    "            out_file.close\n",
    "\n",
    "            important_information = [information[0],information[10], urls[j]]\n",
    "\n",
    "            # Append data to .tsv with only 3 important fields\n",
    "            with open(path_documents_file, 'a', encoding='utf-8') as f:\n",
    "                tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "                # Write information \n",
    "                tsv_writer.writerow(important_information)\n",
    "\n",
    "            f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d6012-2187-4e47-afd7-d35704c9927a",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdebb8a-6f82-44ae-9102-f162ac783767",
   "metadata": {},
   "source": [
    "### Create pandas dataframe with important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6d141a-bde4-42a1-8b37-3d432376f09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_from_3_information_file(path_documents_file):\n",
    "    original_df = pd.read_csv(path_documents_file, sep='\\t')\n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0638e6-a65f-46e9-83d2-e9db8f4271ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text cleaning:\n",
    "\n",
    "In order we will do the following operations:\n",
    "\n",
    "- Remove \"[Written by MAL Rewrite]\" at the end of each description\n",
    "\n",
    "- Remove Contractions (for example \"won't\" become \"will not\" and \"don't\" become \"do not\")\n",
    "\n",
    "- Make all characters lower case (for example \"Hello\" become \"hello\")\n",
    "\n",
    "- Remove dashes\n",
    "\n",
    "- Remove ordinal numbers (for example 1st, 2nd, ...)\n",
    "\n",
    "- Remove stopwords (adding to stopwords frequent words that appear in descriptions, for example \"character\", \"end\") \n",
    "\n",
    "- Remove punctuation\n",
    "\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75af6b51-145e-4ca3-a2b8-200a1580b9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_written_mal_rewrite(text):\n",
    "    return text.replace('[Written by MAL Rewrite]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cab2c5f-e631-4e20-81fa-3c8886c7d458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    # Create patterns\n",
    "    patterns = [\n",
    "       (r'won\\'t', 'will not'),\n",
    "       (r'can\\'t', 'cannot'),\n",
    "       (r'i\\'m', 'i am'),\n",
    "       (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "       (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "       (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "       (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "       (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    ]\n",
    "    \n",
    "    # Compile patterns with re.compile()\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    \n",
    "    # Substitute words\n",
    "    for (pattern, repl) in patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653284d5-75d1-46b8-bea7-79d60023679a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016de28f-976c-454b-a9f5-45c2f950c916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_dashes(text):\n",
    "    return text.replace('—',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da14f53c-abf7-4365-832e-71e7b7a46037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_ordinal_num(text):\n",
    "    text = re.sub('[\\d]+(st|nd|rd|th)', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1821eb6-4bde-4e02-88ad-c7ac142b6d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return [w for s in sent_tokenize(text) for w in word_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e75a3d38-5ae6-442c-b33a-430f6ac2e281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(text)\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54be1607-79ac-46d3-b797-084cd3ebf703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "398b8cd6-1a36-40ef-a6de-b86d39b8baff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f201811-b9bb-452b-8df2-f323a601d265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Text cleaning\n",
    "    text = remove_written_mal_rewrite(text)\n",
    "    text = remove_contractions(text)\n",
    "    text = to_lower_case(text)\n",
    "    text = replace_dashes(text)\n",
    "    text = remove_ordinal_num(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9fe2868-d1a0-49cd-b5b2-729e731b8a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_df(original_df):\n",
    "    df_descriptions = original_df['animeDescription']\n",
    "    df_descriptions = pd.DataFrame(df_descriptions)\n",
    "    df_cleaned = df_descriptions.applymap(lambda x : clean_text(x), na_action='ignore')\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272c901-7461-4036-a07f-bd61a87963fa",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b42b36-14da-4b5e-a6fc-eb3df77eab8f",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfdbf4-17ec-4e2c-aa37-d23d9abbfcd8",
   "metadata": {},
   "source": [
    "#### Create vocabulary and save it in a file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25182171-8082-42bd-b321-78f5b90b3785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocabulary(path_vocabulary_file):    \n",
    "    # Create a list \n",
    "    df_words = df_descriptions.applymap(lambda x : set(tokenize_text(x)), na_action='ignore')\n",
    "    words_union = set()\n",
    "\n",
    "    for row in df_words['animeDescription']:\n",
    "        words_union = words_union.union(set(row))\n",
    "\n",
    "    words_list = list(words_union)\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocabulary = { words_list[i] : i for i in range(len(words_list)) }\n",
    "\n",
    "    # Write the file\n",
    "    with open(path_vocabulary_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(vocabulary, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef495a-5c83-450b-bd0d-b7a0e1991d1d",
   "metadata": {},
   "source": [
    "#### Create index and save it in a file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda89664-821a-417b-8bb6-e94a21385d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_index(path_index_file):\n",
    "    # Create the index with (id_word : [doc1, doc2, ...])\n",
    "    index = {}\n",
    "    for n_row in range(df_words['animeDescription'].shape[0]):\n",
    "        for (k,v) in vocabulary.items():\n",
    "            if v not in index.keys():\n",
    "                index[v] = []\n",
    "            if k in df_words['animeDescription'][n_row]:\n",
    "                # n_row+1 becouse n_anime is n_row_in_dataset + 1\n",
    "                index[v].append(n_row+1)\n",
    "\n",
    "    # Write the file\n",
    "    with open(path_index_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(index, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3615e-4624-43ac-8a15-df71b68bc3fd",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2b219-b74a-4cfa-981e-2003c29a2d5e",
   "metadata": {},
   "source": [
    "#### Load vocabulary.json and index.json as python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffc4fc1-eb34-4290-b43d-19c2de5c07f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_vocabulary_and_index(path_vocabulary_file, path_index_file):\n",
    "    # Vocabulary\n",
    "    with open(path_vocabulary_file, 'r', encoding='utf-8') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    f.close\n",
    "\n",
    "    # Index\n",
    "    with open(path_index_file, 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "    f.close\n",
    "    \n",
    "    return vocabulary, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033b19f-707d-47e3-8e85-f688d8371048",
   "metadata": {},
   "source": [
    "#### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfc19fb6-79af-429e-9d94-a30c645c67e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_query(query, vocabulary,index): #Perche importiamo questo index  ?\n",
    "    # Query\n",
    "    query = clean_text(query).split(\" \")\n",
    "\n",
    "    # Trasform every word of the query in the corrispondant id\n",
    "    id_words_query = [vocabulary[word] for word in query if word in vocabulary.keys()]\n",
    "\n",
    "    documents_set = set()\n",
    "    for id_word in id_words_query:\n",
    "        if documents_set == set():\n",
    "            documents_set.update(index[str(id_word)])\n",
    "        else:\n",
    "            documents_set = documents_set.intersection(set(index[str(id_word)]))\n",
    "            if documents_set == set():\n",
    "                break\n",
    "\n",
    "    documents = list(documents_set)\n",
    "    documents_in_dataframe = [x-1 for x in documents]\n",
    "\n",
    "    # Return rows\n",
    "    original_df = df_from_3_information_file(path_documents_file)\n",
    "    return original_df.iloc[documents_in_dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "728add9a-1030-4bb7-9736-fb0e872bb11e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_documents_file='documents.tsv'\n",
    "path_vocabulary_file='vocabulary.json'\n",
    "path_index_file='Uindex.json'\n",
    "vocabulary, index = read_vocabulary_and_index(path_vocabulary_file, path_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11397cd6-c8d5-48cb-ba0c-9b5ccce2e2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabulary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff8c4b31-7072-48f2-86e8-5d8ba30ffa0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "318fa99c-469d-4f79-af78-cb697fe773f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [animeTitle, animeDescription, Url]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = execute_query(\"mentor robot\",vocabulary, index)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7ce3d-b421-4af6-a003-e433da925dad",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eff1a9-8e4b-4f11-b55a-bea2a9697d1a",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index\n",
    "\n",
    "Calculate $tfIdf$ as:\n",
    "$$\n",
    "\\frac{tf_{i,j}}{{|n_{i,j}|}}\n",
    "$$\n",
    "where $tf_{i,j}$ is the number of time the word i is present in document j and ${|n_{i,j}|}$ is the lenght of the document (number of total words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1be5c269-b229-446e-aece-c59b14ca8f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df = df_from_3_information_file(path_documents_file)\n",
    "df_cleaned = clean_df(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce912c6f-1f08-42f1-9084-d52631972a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TF\n",
    "# cacola term frequency per ogni documento, ma la chiave è la parola, non l'id\n",
    "tf = []\n",
    "for num_row in range(len(df_cleaned)):\n",
    "    doc = df_cleaned.animeDescription.loc[num_row].split()\n",
    "    len_doc = len(doc)\n",
    "    counter = Counter(doc)\n",
    "    for item, count in counter.items():\n",
    "        counter[item] /= len_doc\n",
    "    tf.append(counter)\n",
    "tf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "895734fc-5f67-4f98-9125-0ab53529cea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## IDF\n",
    "idf = {}\n",
    "import numpy as np\n",
    "numdocs_cont_j = {}\n",
    "num_docs = len(df_cleaned)\n",
    "for term_id in index:\n",
    "    numdocs_cont_j = len(index[term_id])\n",
    "    idfscore = np.log(num_docs/numdocs_cont_j)\n",
    "    idf[term_id] = idfscore\n",
    "idf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74a0f2df-54e8-4959-bcd0-bf8f0d649612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## per mappare le chiavi di tf in numeri(id)\n",
    "tf_new = []\n",
    "for d in tf:\n",
    "    dic = dict((vocabulary[key], value) for (key, value) in d.items())\n",
    "    tf_new.append(dic)\n",
    "tf_new;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db61560a-66ad-4405-a516-a20d961d3b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for term in idf: #for every term_id\n",
    "    \n",
    "    for i in range(len(tf_new)): #for every doc\n",
    "        if int(term) in tf_new[i].keys(): #if term_id is the key of the document_i\n",
    "            tupla = (i+1, idf[term]*tf_new[i][int(term)]) #note i+1\n",
    "            \n",
    "            if term in inverted_index:\n",
    "                inverted_index[term].append(tupla)\n",
    "            else:\n",
    "                inverted_index[term] = []\n",
    "                inverted_index[term].append(tupla)\n",
    "inverted_index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b97b765d-1237-4d27-ae2d-b2b3679e23c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def docs_contains_query(query,vocabulary, index):\n",
    "    subset = execute_query(query, vocabulary, index)\n",
    "    indexes = [i+1 for i in subset.index]\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d42f489b-c3ce-417a-9857-3a74f9af23a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the numpy array vector for the query with frequency of words. It has the same lenght of the vocabulary and has number != 0 if the word is present in the query\n",
    "def get_query_vector(query, vocabulary):\n",
    "    query = clean_text(query).split(\" \")\n",
    "    query_vector = np.zeros(len(vocabulary.keys()))\n",
    "    for word in query:\n",
    "        query_vector[int(vocabulary[word])] += 1\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44f6bb62-ac17-4ad3-9dcb-ee2666e5e549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "desc = \"mentor robot\"\n",
    "q_vector = get_query_vector(desc, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbaba5d2-85f0-4e88-8e94-4c88ba45aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same method to get the numpy array vector of the document selected \n",
    "def get_doc_vector(n_row_doc, df_cleaned, inverted_index):\n",
    "    doc = df_cleaned.animeDescription.loc[n_row_doc].split(\" \")\n",
    "    doc_vector = np.zeros(len(vocabulary.keys()))\n",
    "    # iterating in vocabulary of terms\n",
    "    for term_id, value in inverted_index.items():\n",
    "        # Iterating in each tuple:\n",
    "        for pair in value:\n",
    "            # if the document in which the word is present is the document that i passed to this function \n",
    "            if pair[0] == n_row_doc+1:\n",
    "                # Update doc_vector with the respective tfidf in the position [term_id]  \n",
    "                doc_vector[int(term_id)] = pair[1]\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08e3700a-348c-4919-909e-370468d11ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = get_doc_vector(0, df_cleaned, inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9d10b52-fab9-47b5-a402-0e8c6723bdaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_score(q_vector, doc_vector):\n",
    "    # Calculate cosine similarity between query vector and the selected document vector \n",
    "    score = np.dot(q_vector, doc_vector)/(math.sqrt(np.linalg.norm(q_vector))*math.sqrt(np.linalg.norm(doc_vector)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdb6db35-26d4-4923-8849-846a194d7956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039943429806120465"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calc_score(q_vector, doc_vector)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61d39582-7d7d-45bf-b118-5d33d0b3454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_similarity_query(query, k, df_cleaned, vocabulary, inverted_index, original_df):\n",
    "    # Initialize the heap\n",
    "    heap = []\n",
    "    # Query vector\n",
    "    q_vector = get_query_vector(query, vocabulary)\n",
    "    \n",
    "    #Manca la funzionee  docs contain query per confrontare solo con i  documenti di interesse \n",
    "    doc_list = docs_contains_query(query,vocabulary, index)\n",
    "    # For all the synopsis in the dataframe\n",
    "    for i in doc_list:\n",
    "        doc_i_vector = get_doc_vector(i, df_cleaned, inverted_index)\n",
    "        score_i = calc_score(q_vector, doc_i_vector)\n",
    "        # Push in the heap the tuple containing (score of i-th document, i)\n",
    "        if score_i  != 0:\n",
    "            heapq.heappush(heap, (score_i, i))\n",
    "            \n",
    "    heap=[heapq.heappop(heap) for i in range(len(heap))]\n",
    "    \n",
    "    if len(heap) > k:\n",
    "        heap=heap[len(heap)-1:len(heap)-k-1:-1]\n",
    "        \n",
    "    else:\n",
    "        heap=heap[k-1:-1:-1]\n",
    "    \n",
    "    # List of indexes of k-documents \n",
    "    doc_rows_list = []\n",
    "    score_list=[]\n",
    "    if heap==[]:\n",
    "        return ('There are not significant similarity')\n",
    "    else:\n",
    "        for j in range(len(heap)):\n",
    "            #tuple_element = heapq.heappop(heap)\n",
    "            doc_rows_list.append(heap[j][1])\n",
    "            score_list.append(heap[j][0])\n",
    "        \n",
    "        sub_df=original_df.iloc[doc_rows_list]\n",
    "\n",
    "        sub_df['Similarity']=score_list #purche l ordine con cui printiamo il sub dataset  sia lo  stesso del vettore doc_rows_list\n",
    "        return  sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c89e505e-8f1d-4e59-a322-a3d678782099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are not significant similarity'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_similarity_query(\"robot\",2,df_cleaned, vocabulary, inverted_index, original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7288b-5d0f-4235-96ec-abbe271d1fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
