{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f13d2",
   "metadata": {},
   "source": [
    "# Homework 3 - Functions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e300f25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import string\n",
    "import json\n",
    "\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english')\n",
    "\n",
    "# Append some recurrent words in synopsis:\n",
    "frequent_words = ['character','characters','end']\n",
    "default_stopwords.extend(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "713c6d86-b4f4-4aae-a531-847a7b5fa464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_list_anime = \"C:/Users/andre/Desktop/Anime/list_anime.txt\"\n",
    "url_of_single_page = \"https://myanimelist.net/topanime.php?limit=\"\n",
    "path_page_general = \"C:/Users/andre/Desktop/Anime/Page\"\n",
    "path_documents_file = \"C:/Users/andre/Desktop/Anime/documents.tsv\"\n",
    "path_vocabulary_file = \"C:/Users/andre/Desktop/Anime/vocabulary.json\"\n",
    "path_index_file = \"C:/Users/andre/Desktop/Anime/index.json\"\n",
    "path_documents_ex_3 = \"C:/Users/andre/Desktop/Anime/documents_ex_3.tsv\"\n",
    "path_vocabulary_title_file = \"C:/Users/andre/Desktop/Anime/vocabulary_title.json\"\n",
    "path_index_title_file =\"C:/Users/andre/Desktop/Anime/index_title.json\"\n",
    "path_inverted_index_file=\"C:/Users/andre/Desktop/Anime/inverted_index.json\"\n",
    "path_inverted_index_title_file = \"C:/Users/andre/Desktop/Anime/inverted_index_title.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74c803",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abc382-bcdd-4ab5-a315-768b8558a616",
   "metadata": {},
   "source": [
    "#### Write file with all anime urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ffca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 19130 anime in total, 383 pages with 50 animes\n",
    "\n",
    "def write_list_anime(path_list_anime, url_of_single_page):\n",
    "    # Number of total pages\n",
    "    n_pages = 383 \n",
    "\n",
    "    with open(path_list_anime, \"w\", encoding='utf-8') as file:\n",
    "\n",
    "        for page in tqdm(range(0, n_pages)):\n",
    "\n",
    "            # URL of the single page \n",
    "            url = url_of_single_page + str(page * 50)\n",
    "\n",
    "            # Get response object\n",
    "            response = r.get(url)\n",
    "\n",
    "            # Get html\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all links iterating through the document\n",
    "            for tag in soup.find_all(\"tr\", class_=\"ranking-list\"):\n",
    "                a_list = tag.find_all('a', class_=\"hoverinfo_trigger fl-l ml12 mr8\" ,href=True)\n",
    "                for a in a_list:\n",
    "                    link = a['href']\n",
    "                    file.write(str(link) + '\\n')\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0d6f",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ce4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_html(path_list_anime, path_page_general):\n",
    "    file = open(path_list_anime, \"r\", encoding='utf-8')\n",
    "\n",
    "    # Initialize counters \n",
    "    counter_anime = 11500\n",
    "    page = 230\n",
    "\n",
    "    # Get html file for every anime (url is a line in the file list_anime) \n",
    "    for line in tqdm(file):\n",
    "        # Every 50 animes the program stops because of the scraping block of the website\n",
    "        time.sleep(1.5)\n",
    "        counter_anime += 1\n",
    "        if (counter_anime%50 == 1):\n",
    "            page +=1        \n",
    "        response = r.get(line)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            os.mkdir(path_page_general + str(page))\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        with open(path_page_general + str(page) + \"/article_\" + str(counter_anime) + \".html\", \"w\", encoding='utf-8') as file:\n",
    "            file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b74d7e-5a7f-40f1-9ae0-6a559da34d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#download_html(path_list_anime, path_page_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438979ef",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bd65c-48df-4bae-acd2-1e6e52af4476",
   "metadata": {},
   "source": [
    "#### Function to retrieve all the information from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9981071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrive information at page i of anime j\n",
    "def retrive_information(i, j):\n",
    "        with open(path_page_general + str(i) + \"/article_\" + str(j) + \".html\", \"r\", encoding='utf-8') as file:\n",
    "    \n",
    "            # Read file and parse html code\n",
    "            content_page = file.read()\n",
    "            soup = BeautifulSoup(content_page, 'html.parser')\n",
    "            \n",
    "            # Retrive information\n",
    "            \n",
    "            animeTitle = soup.find_all(\"h1\", class_=\"title-name h1_bold_none\")[0].find(\"strong\").text\n",
    "        \n",
    "            animeType = soup.find_all('a', href = re.compile(r'type*'))[0].text.strip()\n",
    "            \n",
    "            animeNumEpisode = soup.find(string='Episodes:').next_element.replace(\"\\n\",\"\").strip()\n",
    "            \n",
    "            animeNumMembers = soup.find(string='Members:').next_element.replace(\"\\n\",\"\").replace(',','').replace('#', '').strip()\n",
    "        \n",
    "            animeScore = soup.find('span', {\"itemprop\":\"ratingValue\"}).text.strip()\n",
    "            \n",
    "            animeUsers = soup.find('span', {\"itemprop\":\"ratingCount\"}).text.strip()\n",
    "            \n",
    "            animeRank = soup.find(string='Ranked:').next_element.replace(\"\\n\",\"\").replace('#','').strip()\n",
    "            \n",
    "            animePopularity = soup.find(string='Popularity:').next_element.replace(\"\\n\",\"\").replace('#', '').strip()\n",
    "            \n",
    "            animeDescription = soup.find('p',{'itemprop':'description'}).text.replace('\\n', '').replace('  ', '').strip()\n",
    "\n",
    "    \n",
    "            # Iterate in di with dates   \n",
    "            dates_div = soup.find_all(\"div\", class_=\"spaceit_pad\")\n",
    "            for i in range(0, len(dates_div)):\n",
    "        \n",
    "                # Dates\n",
    "                if dates_div[i].find(\"span\", string='Aired:') != None:\n",
    "                    dates = dates_div[i].contents[2].strip().split(\"to\")\n",
    "                    try:\n",
    "                        releaseDate = datetime.strptime(dates[0].strip(), '%b %d, %Y').date()\n",
    "                    except:\n",
    "                        releaseDate = None\n",
    "                    try:\n",
    "                        endDate = datetime.strptime(dates[1].strip(), '%b %d, %Y').date()\n",
    "                    except:  \n",
    "                        endDate = None\n",
    "    \n",
    "            # Anime Description\n",
    "            animeDescription = soup.find(\"p\", itemprop='description').text\n",
    "    \n",
    "            # Related Animes\n",
    "            try:\n",
    "                related_anime_table = soup.find(\"table\", class_=\"anime_detail_related_anime\")\n",
    "                links_list = [link.text for link in related_anime_table.find_all(\"a\")]\n",
    "                animeRelated = list(set(links_list))\n",
    "            except:\n",
    "                animeRelated = None\n",
    "    \n",
    "            # Characters\n",
    "            animeCharacters = soup.find_all(\"h3\", class_=\"h3_characters_voice_actors\")\n",
    "            animeCharacters = [character.text for character in animeCharacters]\n",
    "    \n",
    "            # Voices\n",
    "            animeVoices = soup.find_all(\"td\", class_=\"va-t ar pl4 pr4\")\n",
    "            animeVoices = [actor.find(\"a\").text for actor in animeVoices]\n",
    "    \n",
    "            a_no = []\n",
    "            total = []\n",
    "            result = []\n",
    "            animeStaff = []\n",
    "    \n",
    "            # Staff \n",
    "            if animeCharacters == [] and animeVoices == []:\n",
    "                div = soup.find_all(\"div\", class_=\"detail-characters-list clearfix\")[0]\n",
    "                tds = div.find_all(\"td\", class_=lambda x: x != 'ac borderClass' and x != 'ac' and x == 'borderClass')\n",
    "    \n",
    "                smalls = div.find_all(\"small\")\n",
    "    \n",
    "                for td in tds:\n",
    "                    a_no = set(list(td.find_all(\"a\", class_=\"fw-n\")))\n",
    "                    total = set(list(td.find_all(\"a\")))\n",
    "                    if list(total.difference(a_no)) != []:\n",
    "                        result.append(list(total.difference(a_no)))\n",
    "    \n",
    "                for (s, role) in zip(result, smalls):\n",
    "                    s = str(s).strip(\"[\").strip(\"]\")\n",
    "                    htmlTag = BeautifulSoup(s, \"html.parser\")\n",
    "                    animeStaff.append([htmlTag.text, role.text])\n",
    "                \n",
    "            # There are no staff\n",
    "            elif len(soup.find_all(\"div\", class_=\"detail-characters-list clearfix\")) < 2:\n",
    "                animeStaff = []\n",
    "                \n",
    "            # There are one of animeCharacters or animeVoices\n",
    "            else:\n",
    "                div = soup.find_all(\"div\", class_=\"detail-characters-list clearfix\")[1]\n",
    "                \n",
    "                tds = div.find_all(\"td\", class_=lambda x: x != 'ac borderClass' and x != 'ac' and x == 'borderClass')\n",
    "\n",
    "                smalls = div.find_all(\"small\")\n",
    "\n",
    "                for td in tds:\n",
    "                    a_no = set(list(td.find_all(\"a\", class_=\"fw-n\")))\n",
    "                    total = set(list(td.find_all(\"a\")))\n",
    "                    if list(total.difference(a_no)) != []:\n",
    "                        result.append(list(total.difference(a_no)))\n",
    "\n",
    "                for (s, role) in zip(result, smalls):\n",
    "                    s = str(s).strip(\"[\").strip(\"]\")\n",
    "                    htmlTag = BeautifulSoup(s, \"html.parser\")\n",
    "                    animeStaff.append([htmlTag.text, role.text])\n",
    "            \n",
    "        # Create list with overall information\n",
    "        final_information = [animeTitle, animeType,animeNumEpisode,releaseDate,endDate,\n",
    "                         animeNumMembers, animeScore,animeUsers,animeRank,animePopularity,\n",
    "                         animeDescription,animeRelated,animeCharacters,animeVoices,animeStaff]\n",
    "        return final_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951208c-42e9-4324-802c-465639f79315",
   "metadata": {},
   "source": [
    "#### Create a .tsv file for each anime and a .tsv file with 3 most important information for all: Title, Description, Url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73b577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tsvs(path_list_anime, path_page_general, path_documents_file, path_documents_ex_3):\n",
    "    # Create .tsv file\n",
    "    n_pages = 384\n",
    "\n",
    "    # Collecting all Urls\n",
    "    with open(path_list_anime, 'r', encoding='utf-8') as f_urls:\n",
    "        urls = f_urls.readlines()\n",
    "    f_urls.close\n",
    "\n",
    "    # Create .tsv with only 3 important fields\n",
    "    with open(path_documents_file, 'w', encoding='utf-8') as f:\n",
    "        tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "        # Write haeders\n",
    "        tsv_writer.writerow(['animeTitle','animeDescription','Url'])\n",
    "\n",
    "    f.close\n",
    "    \n",
    "    # Create .tsv unique with only 4 fields related to excercise 3\n",
    "    with open(path_documents_ex_3, 'w', encoding='utf-8') as f:\n",
    "        tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "        # Write headers\n",
    "        tsv_writer.writerow(['animeTitle','animeDescription','Url', 'animeScore'])\n",
    "\n",
    "    f.close\n",
    "\n",
    "    # Iterating through pages and then animes\n",
    "    for i in range(1, n_pages):\n",
    "        \n",
    "        # Start and end computed with respect to the \n",
    "        start_iterate_anime = (i-1)*50+1\n",
    "        end_iterate_anime = ((i-1)*50+1)+50\n",
    "        for j in range(start_iterate_anime, end_iterate_anime):\n",
    "\n",
    "            # Write for each anime a .tsv file\n",
    "            with open(path_page_general + str(i) + \"/anime_\" + str(j)+ \".tsv\", 'w', encoding='utf-8') as out_file:\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "                # Write haeders\n",
    "                tsv_writer.writerow(['animeTitle', 'animeType','animeNumEpisode','releaseDate','endDate',\n",
    "                                     'animeNumMembers', 'animeScore','animeUsers','animeRank','animePopularity',\n",
    "                                     'animeDescription','animeRelated','animeCharacters','animeVoices','animeStaff'])\n",
    "\n",
    "                # Write information\n",
    "                information = retrive_information(i, j) \n",
    "                tsv_writer.writerow(information)\n",
    "            out_file.close\n",
    "\n",
    "            important_information = [information[0],information[10], urls[j]]\n",
    "            important_information_ex_3 = [information[0],information[10], urls[j], information[6]]\n",
    "\n",
    "            # Append data to unique .tsv with only 3 important fields\n",
    "            with open(path_documents_file, 'a', encoding='utf-8') as f:\n",
    "                tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "                # Write information \n",
    "                tsv_writer.writerow(important_information)\n",
    "\n",
    "            f.close\n",
    "            \n",
    "            # Append data to unique .tsv with only 4 fields related to excercise 3\n",
    "            with open(path_documents_ex_3, 'a', encoding='utf-8') as f:\n",
    "                tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "                # Write information \n",
    "                tsv_writer.writerow(important_information_ex_3)\n",
    "\n",
    "            f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36346043-8aa6-429f-90cb-cfc29c5d95fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_tsvs(path_list_anime, path_page_general, path_documents_file, path_documents_ex_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44617662-8a23-4d8c-95d7-fd2f1eed2900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = df_from_4_information_file(path_documents_ex_3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d6012-2187-4e47-afd7-d35704c9927a",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdebb8a-6f82-44ae-9102-f162ac783767",
   "metadata": {},
   "source": [
    "### Create pandas dataframe with important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6d141a-bde4-42a1-8b37-3d432376f09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_from_3_information_file(path_documents_file):\n",
    "    original_df = pd.read_csv(path_documents_file, sep='\\t')\n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a101990-2ab5-4c22-a894-ee1561f65f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_from_4_information_file(path_documents_ex_3):\n",
    "    original_df = pd.read_csv(path_documents_ex_3, sep='\\t')\n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0638e6-a65f-46e9-83d2-e9db8f4271ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text cleaning:\n",
    "\n",
    "In order we will do the following operations:\n",
    "\n",
    "- Remove \"[Written by MAL Rewrite]\" at the end of each description\n",
    "\n",
    "- Remove Contractions (for example \"won't\" become \"will not\" and \"don't\" become \"do not\")\n",
    "\n",
    "- Make all characters lower case (for example \"Hello\" become \"hello\")\n",
    "\n",
    "- Remove dashes\n",
    "\n",
    "- Remove ordinal numbers (for example 1st, 2nd, ...)\n",
    "\n",
    "- Remove stopwords (adding to stopwords frequent words that appear in descriptions, for example \"character\", \"end\") \n",
    "\n",
    "- Remove punctuation\n",
    "\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75af6b51-145e-4ca3-a2b8-200a1580b9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_written_mal_rewrite(text):\n",
    "    return text.replace('[Written by MAL Rewrite]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cab2c5f-e631-4e20-81fa-3c8886c7d458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    # Create patterns\n",
    "    patterns = [\n",
    "       (r'won\\'t', 'will not'),\n",
    "       (r'can\\'t', 'cannot'),\n",
    "       (r'i\\'m', 'i am'),\n",
    "       (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "       (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "       (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "       (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "       (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    ]\n",
    "    \n",
    "    # Compile patterns with re.compile()\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    \n",
    "    # Substitute words\n",
    "    for (pattern, repl) in patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653284d5-75d1-46b8-bea7-79d60023679a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016de28f-976c-454b-a9f5-45c2f950c916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_dashes(text):\n",
    "    return text.replace('—',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da14f53c-abf7-4365-832e-71e7b7a46037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_ordinal_num(text):\n",
    "    text = re.sub('[\\d]+(st|nd|rd|th)', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1821eb6-4bde-4e02-88ad-c7ac142b6d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return [w for s in sent_tokenize(text) for w in word_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75a3d38-5ae6-442c-b33a-430f6ac2e281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(text)\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54be1607-79ac-46d3-b797-084cd3ebf703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398b8cd6-1a36-40ef-a6de-b86d39b8baff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f201811-b9bb-452b-8df2-f323a601d265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Text cleaning\n",
    "    text = remove_written_mal_rewrite(text)\n",
    "    text = remove_contractions(text)\n",
    "    text = to_lower_case(text)\n",
    "    text = replace_dashes(text)\n",
    "    text = remove_ordinal_num(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9fe2868-d1a0-49cd-b5b2-729e731b8a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_df(original_df, column_type):\n",
    "    df_descriptions = original_df[column_type]\n",
    "    df_descriptions = pd.DataFrame(df_descriptions)\n",
    "    df_cleaned = df_descriptions.applymap(lambda x : clean_text(x), na_action='ignore')\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272c901-7461-4036-a07f-bd61a87963fa",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b42b36-14da-4b5e-a6fc-eb3df77eab8f",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfdbf4-17ec-4e2c-aa37-d23d9abbfcd8",
   "metadata": {},
   "source": [
    "#### Create vocabulary and save it in a file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25182171-8082-42bd-b321-78f5b90b3785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocabulary(path_vocabulary_file, df_descriptions):    \n",
    "    # Create a list \n",
    "    df_words = df_descriptions.applymap(lambda x : set(tokenize_text(x)), na_action='ignore')\n",
    "    words_union = set()\n",
    "\n",
    "    for row in df_words['animeDescription']:\n",
    "        words_union = words_union.union(set(row))\n",
    "\n",
    "    words_list = list(words_union)\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocabulary = { words_list[i] : i for i in range(len(words_list)) }\n",
    "\n",
    "    # Write the file\n",
    "    with open(path_vocabulary_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(vocabulary, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef495a-5c83-450b-bd0d-b7a0e1991d1d",
   "metadata": {},
   "source": [
    "#### Create index and save it in a file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dda89664-821a-417b-8bb6-e94a21385d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_index(vocabulary, path_index_file, df_descriptions):\n",
    "    # Create the index with (id_word : [doc1, doc2, ...])\n",
    "    df_words = df_descriptions.applymap(lambda x : set(tokenize_text(x)), na_action='ignore')\n",
    "    index = {}\n",
    "    for n_row in range(df_words['animeDescription'].shape[0]):\n",
    "        for (k,v) in vocabulary.items():\n",
    "            if v not in index.keys():\n",
    "                index[v] = []\n",
    "            if k in df_words['animeDescription'][n_row]:\n",
    "                # n_row+1 becouse n_anime is n_row_in_dataset + 1\n",
    "                index[v].append(n_row+1)\n",
    "\n",
    "    # Write the file\n",
    "    with open(path_index_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(index, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753079a-88be-4cce-ac86-9db5df299dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df = df_from_3_information_file(path_documents_file)\n",
    "original_df = clean_df(original_df, column_type)\n",
    "df_descriptions = pd.Series.to_frame(original_df.animeDescription)\n",
    "write_vocabulary(path_vocabulary_file, df_descriptions)\n",
    "vocabulary = read_vocabulary(path_vocabulary_file)\n",
    "write_index(vocabulary, path_index_file, df_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0a883-8bbb-4835-8e86-a1a7e900b82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = read_index(path_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5765251-3bf9-4baa-9132-84ed6abdb470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df.animeDescription[0]\n",
    "robot = vocabulary[\"robot\"]\n",
    "robot = index[str(1710)]\n",
    "robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3615e-4624-43ac-8a15-df71b68bc3fd",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2b219-b74a-4cfa-981e-2003c29a2d5e",
   "metadata": {},
   "source": [
    "#### Load vocabulary.json and index.json as python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a4facbe-e05a-4a9a-96a6-f780cfa6dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocabulary(path_vocabulary_file):\n",
    "     # Vocabulary\n",
    "    with open(path_vocabulary_file, 'r', encoding='utf-8') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    f.close\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d176d60-cf5e-40a6-92ef-24ec8d633006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index(path_index_file):\n",
    "    # Index\n",
    "    with open(path_index_file, 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "    f.close\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154e8bc-0599-48ca-9fa9-c78ec988f8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3033b19f-707d-47e3-8e85-f688d8371048",
   "metadata": {},
   "source": [
    "#### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bfc19fb6-79af-429e-9d94-a30c645c67e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_query(query, vocabulary, index, path_documents_file):\n",
    "    # Query\n",
    "    query = clean_text(query).split(\" \")\n",
    "\n",
    "    # Trasform every word of the query in the corrispondant id\n",
    "    id_words_query = []\n",
    "    for word in query:\n",
    "        if word in vocabulary.keys():\n",
    "            id_words_query.append(vocabulary[word])\n",
    "        else:\n",
    "            id_words_query = []\n",
    "            break\n",
    "\n",
    "    documents_set = set()\n",
    "    for id_word in id_words_query:\n",
    "        if documents_set == set():\n",
    "            documents_set.update(index[str(id_word)])\n",
    "        else:\n",
    "            documents_set = documents_set.intersection(set(index[str(id_word)]))\n",
    "            if documents_set == set():\n",
    "                break\n",
    "\n",
    "    documents = list(documents_set)\n",
    "    documents_in_dataframe = [x-1 for x in documents]\n",
    "\n",
    "    # Return rows\n",
    "    original_df = df_from_3_information_file(path_documents_file)\n",
    "    return original_df.iloc[documents_in_dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "318fa99c-469d-4f79-af78-cb697fe773f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/28977/Gintama°\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>The Abyss—a gaping chasm stretching down into ...</td>\n",
       "      <td>https://myanimelist.net/anime/2921/Ashita_no_J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Neon Genesis Evangelion: The End of Evangelion</td>\n",
       "      <td>Shinji Ikari is left emotionally comatose afte...</td>\n",
       "      <td>https://myanimelist.net/anime/38474/Yuru_Camp△...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Nichijou</td>\n",
       "      <td>Nichijou primarily focuses on the daily antics...</td>\n",
       "      <td>https://myanimelist.net/anime/31181/Owarimonog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Tengen Toppa Gurren Lagann</td>\n",
       "      <td>Simon and Kamina were born and raised in a dee...</td>\n",
       "      <td>https://myanimelist.net/anime/34591/Natsume_Yu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         animeTitle  \\\n",
       "0                  Fullmetal Alchemist: Brotherhood   \n",
       "45                                    Made in Abyss   \n",
       "90   Neon Genesis Evangelion: The End of Evangelion   \n",
       "124                                        Nichijou   \n",
       "62                       Tengen Toppa Gurren Lagann   \n",
       "\n",
       "                                      animeDescription  \\\n",
       "0    After a horrific alchemy experiment goes wrong...   \n",
       "45   The Abyss—a gaping chasm stretching down into ...   \n",
       "90   Shinji Ikari is left emotionally comatose afte...   \n",
       "124  Nichijou primarily focuses on the daily antics...   \n",
       "62   Simon and Kamina were born and raised in a dee...   \n",
       "\n",
       "                                                   Url  \n",
       "0     https://myanimelist.net/anime/28977/Gintama°\\r\\n  \n",
       "45   https://myanimelist.net/anime/2921/Ashita_no_J...  \n",
       "90   https://myanimelist.net/anime/38474/Yuru_Camp△...  \n",
       "124  https://myanimelist.net/anime/31181/Owarimonog...  \n",
       "62   https://myanimelist.net/anime/34591/Natsume_Yu...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = execute_query(\"robot\", vocabulary, index, path_documents_file)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7ce3d-b421-4af6-a003-e433da925dad",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eff1a9-8e4b-4f11-b55a-bea2a9697d1a",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index\n",
    "\n",
    "Calculate $tfIdf$ as:\n",
    "$$\n",
    "\\frac{tf_{i,j}}{{|n_{i,j}|}}\n",
    "$$\n",
    "where $tf_{i,j}$ is the number of time the word i is present in document j and ${|n_{i,j}|}$ is the lenght of the document (number of total words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5c269-b229-446e-aece-c59b14ca8f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df = df_from_3_information_file(path_documents_file)\n",
    "df_cleaned = clean_df(original_df, column_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bddf4cb1-f9c9-4394-b072-da121393d857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TF\n",
    "# cacola term frequency per ogni documento, ma la chiave è la parola, non l'id\n",
    "def write_inverted_index (original_df, vocabulary, index, column_type, path):\n",
    "    df = clean_df(original_df, column_type)\n",
    "    tf = []\n",
    "    for num_row in range(len(df)):\n",
    "        doc = df[column_type].loc[num_row].split()\n",
    "        len_doc = len(doc)\n",
    "        counter = Counter(doc)\n",
    "        for item, count in counter.items():\n",
    "            counter[item] /= len_doc\n",
    "        tf.append(counter)\n",
    "\n",
    "    ## per mappare le chiavi di tf in numeri(id)\n",
    "    tf_new = []\n",
    "    for d in tf:\n",
    "        dic = dict((vocabulary[key], value) for (key, value) in d.items())\n",
    "        tf_new.append(dic)\n",
    "        \n",
    "    idf = {}\n",
    "\n",
    "    numdocs_cont_j = {}\n",
    "    num_docs = len(df)\n",
    "    for term_id in index:\n",
    "        numdocs_cont_j = len(index[term_id])\n",
    "        idfscore = np.log(num_docs/numdocs_cont_j)\n",
    "        idf[term_id] = idfscore\n",
    "\n",
    "    term_id = {}\n",
    "    for term in idf: #for every term_id\n",
    "        for i in range(len(tf_new)): #for every doc\n",
    "            if int(term) in tf_new[i].keys(): #if term_id is the key of the document_i\n",
    "                tupla = (i+1, idf[term]*tf_new[i][int(term)])\n",
    "\n",
    "                if term in term_id:\n",
    "                    term_id[term].append(tupla)\n",
    "                else:\n",
    "                    term_id[term] = []\n",
    "                    term_id[term].append(tupla)\n",
    "    # Write the file\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(term_id, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d748df95-4cec-4149-b0d5-a27572ad152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inverted_index(path_inverted_index_file):\n",
    "    # Index\n",
    "    with open(path_inverted_index_file, 'r', encoding='utf-8') as f:\n",
    "        inverted_index = json.load(f)\n",
    "    f.close\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b97b765d-1237-4d27-ae2d-b2b3679e23c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def docs_contains_query(query, vocabulary, index, path_documents_file):\n",
    "    subset = execute_query(query, vocabulary, index, path_documents_file)\n",
    "    indexes = [i for i in subset.index]\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d42f489b-c3ce-417a-9857-3a74f9af23a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the numpy array vector for the query with frequency of words. It has the same lenght of the vocabulary and has number != 0 if the word is present in the query\n",
    "def get_query_vector(query, vocabulary):\n",
    "    query = clean_text(query).split(\" \")\n",
    "    query_vector = np.zeros(len(vocabulary.keys()))\n",
    "    for word in query:\n",
    "        if word in vocabulary.keys():\n",
    "            query_vector[int(vocabulary[word])] += 1\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6bb62-ac17-4ad3-9dcb-ee2666e5e549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "desc = \"robot\"\n",
    "q_vector = get_query_vector(desc, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbaba5d2-85f0-4e88-8e94-4c88ba45aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same method to get the numpy array vector of the document selected \n",
    "def get_doc_vector(n_row_doc, df_cleaned, vocabulary, inverted_index, column_type):\n",
    "    doc = df_cleaned[column_type].loc[n_row_doc].split(\" \")\n",
    "    doc_vector = np.zeros(len(vocabulary.keys()))\n",
    "    # iterating in vocabulary of terms\n",
    "    for term_id, value in inverted_index.items():\n",
    "        # Iterating in each tuple:\n",
    "        for pair in value:\n",
    "            # if the document in which the word is present is the document that i passed to this function \n",
    "            if pair[0] == n_row_doc+1:\n",
    "                # Update doc_vector with the respective tfidf in the position [term_id]  \n",
    "                doc_vector[int(term_id)] = pair[1]\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3700a-348c-4919-909e-370468d11ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_vector = get_doc_vector(0, df_cleaned, vocabulary, inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d10b52-fab9-47b5-a402-0e8c6723bdaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_score(q_vector, doc_vector):\n",
    "    # Calculate cosine similarity between query vector and the selected document vector \n",
    "    score = np.dot(q_vector, doc_vector)/(math.sqrt(np.linalg.norm(q_vector))*math.sqrt(np.linalg.norm(doc_vector)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6db35-26d4-4923-8849-846a194d7956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = calc_score(q_vector, doc_vector)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "61d39582-7d7d-45bf-b118-5d33d0b3454a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_similarity_query(query, k, df_cleaned, vocabulary, index, inverted_index, original_df, path_documents_file, column_type):\n",
    "    # Initialize the heap\n",
    "    heap = []\n",
    "    # Query vector\n",
    "    q_vector = get_query_vector(query, vocabulary)\n",
    "    \n",
    "    #Manca la funzionee  docs contain query per confrontare solo con i  documenti di interesse \n",
    "    doc_list = docs_contains_query(query,vocabulary, index, path_documents_file)\n",
    "    # For all the synopsis in the dataframe\n",
    "    for i in doc_list:\n",
    "        doc_i_vector = get_doc_vector(i, df_cleaned, vocabulary, inverted_index, column_type)\n",
    "        score_i = calc_score(q_vector, doc_i_vector)\n",
    "        \n",
    "        # Push in the heap the tuple containing (score of i-th document, i)\n",
    "        if score_i  != 0:\n",
    "            heapq.heappush(heap, (score_i, i))\n",
    "    if heap==[]:\n",
    "        return original_df.iloc[[]]\n",
    "    else:\n",
    "        heapq._heapify_max(heap)\n",
    "        if len(heap) > k:\n",
    "            heap=[heapq.heappop(heap) for i in range(k)]\n",
    "        \n",
    "    # List of indexes of k-documents \n",
    "    \n",
    "    \n",
    "        doc_rows_list = []\n",
    "        score_list=[]\n",
    "        for j in range(len(heap)):\n",
    "            #tuple_element = heapq.heappop(heap)\n",
    "            doc_rows_list.append(heap[j][1])\n",
    "            score_list.append(heap[j][0])\n",
    "        \n",
    "        sub_df=original_df.iloc[doc_rows_list]\n",
    "        name_column = 'Similarity ' + column_type\n",
    "        sub_df[name_column]=np.array(score_list,dtype='float') #purche l ordine con cui printiamo il sub dataset  sia lo  stesso del vettore doc_rows_list\n",
    "        return  sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d45b14f6-aad6-42bd-9511-8a007198ee5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "execute_similarity_query() missing 3 required positional arguments: 'original_df', 'path_documents_file', and 'column_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17628/3961971190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexecute_similarity_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"robot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverted_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: execute_similarity_query() missing 3 required positional arguments: 'original_df', 'path_documents_file', and 'column_type'"
     ]
    }
   ],
   "source": [
    "execute_similarity_query(\"robot\", 5, df_cleaned, vocabulary, inverted_index, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08117f73-325d-4c3c-bf46-175268037e92",
   "metadata": {},
   "source": [
    "### 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c4a80-0141-466e-9b56-6ca2e8c90f75",
   "metadata": {},
   "source": [
    "#### Explanation of the new score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5017f0aa-37ad-43c8-a4e6-6bec0e53ad33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocabulary_title(path_vocabulary_title_file, df_title):    \n",
    "    # Create a list \n",
    "    df_words = df_title.applymap(lambda x : set(tokenize_text(x)), na_action='ignore')\n",
    "    words_union = set()\n",
    "\n",
    "    for row in df_words['animeTitle']:\n",
    "        words_union = words_union.union(set(row))\n",
    "\n",
    "    words_list = list(words_union)\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocabulary_title = { words_list[i] : i for i in range(len(words_list)) }\n",
    "\n",
    "    # Write the file\n",
    "    with open(path_vocabulary_title_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(vocabulary_title, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa250659-ed01-401d-8bbe-675aacdb4d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_index_title(vocabulary_title, path_index_title_file, df_title):\n",
    "    # Create the index with (id_word : [doc1, doc2, ...])\n",
    "    df_words = df_title.applymap(lambda x : set(tokenize_text(x)), na_action='ignore')\n",
    "    index_title = {}\n",
    "    for n_row in range(df_words['animeTitle'].shape[0]):\n",
    "        for (k,v) in vocabulary_title.items():\n",
    "            if v not in index_title.keys():\n",
    "                index_title[v] = []\n",
    "            if k in df_words['animeTitle'][n_row]:\n",
    "                # n_row+1 becouse n_anime is n_row_in_dataset + 1\n",
    "                index_title[v].append(n_row+1)\n",
    "\n",
    "    # Write the file\n",
    "    with open(path_index_title_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(index_title, file)\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c8cdb-cd31-4012-a307-734f413cc115",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df_from_4_information_file(path_documents_ex_3)\n",
    "df_cleaned = clean_df_title(original_df, column_type)\n",
    "df_title = pd.Series.to_frame(df_cleaned.animeTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6f4f3-7fda-41c4-826e-9d1b20ade807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_vocabulary_title(path_vocabulary_title_file, df_title)\n",
    "vocabulary_title = read_vocabulary(path_vocabulary_title_file)\n",
    "write_index_title(vocabulary_title, path_index_title_file, df_title)\n",
    "index_title = read_index(path_index_title_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d4ce1-fa0d-4670-9912-e4da835da9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_title = execute_query(\"shiro\", vocabulary_title, index_title, path_documents_ex_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f83931-31c5-4b0f-8b84-1fd96f132492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverted_index_title = inverted_index(df_cleaned, vocabulary_title, \"animeTitle\")\n",
    "inverted_index_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8d7636cf-bcb7-443d-a0a4-9505757a73e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>animeScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [animeTitle, animeDescription, Url, animeScore]\n",
       "Index: []"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"robot\"\n",
    "k = 2\n",
    "original_df = df_from_4_information_file(path_documents_ex_3)\n",
    "df_cleaned = clean_df(original_df, \"animeTitle\")\n",
    "\n",
    "vocabulary_title = read_vocabulary(path_vocabulary_title_file)\n",
    "index_title = read_index(path_index_title_file)\n",
    "invert_index_title = inverted_index(original_df, vocabulary_title, index_title, \"animeTitle\")\n",
    "\n",
    "result1 = execute_similarity_query(query, k, df_cleaned, vocabulary_title,index_title , invert_index_title, original_df, path_documents_file, \"animeTitle\")\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "385af006-fad2-485f-9b77-c5b2b8753ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>animeScore</th>\n",
       "      <th>Similarity animeDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Nichijou</td>\n",
       "      <td>Nichijou primarily focuses on the daily antics...</td>\n",
       "      <td>https://myanimelist.net/anime/31181/Owarimonog...</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.080656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/28977/Gintama°\\r\\n</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.049967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           animeTitle  \\\n",
       "124                          Nichijou   \n",
       "0    Fullmetal Alchemist: Brotherhood   \n",
       "\n",
       "                                      animeDescription  \\\n",
       "124  Nichijou primarily focuses on the daily antics...   \n",
       "0    After a horrific alchemy experiment goes wrong...   \n",
       "\n",
       "                                                   Url  animeScore  \\\n",
       "124  https://myanimelist.net/anime/31181/Owarimonog...        8.46   \n",
       "0     https://myanimelist.net/anime/28977/Gintama°\\r\\n        9.16   \n",
       "\n",
       "     Similarity animeDescription  \n",
       "124                     0.080656  \n",
       "0                       0.049967  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"robot\"\n",
    "k = 2\n",
    "original_df = df_from_4_information_file(path_documents_ex_3)\n",
    "df_cleaned = clean_df(original_df, \"animeDescription\")\n",
    "\n",
    "vocabulary = read_vocabulary(path_vocabulary_file)\n",
    "index = read_index(path_index_file)\n",
    "invert_index = inverted_index (original_df, vocabulary,index, \"animeDescription\")\n",
    "\n",
    "result2 = execute_similarity_query(query, k, df_cleaned, vocabulary,index , invert_index, original_df, path_documents_file, \"animeDescription\")\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "db885e78-6a7d-4569-8f19-79182e32b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_new_score_query(query, k, vocabulary, index, inverted_index, vocabulary_title, index_title, invert_index_title, \n",
    "                            path_documents_file, prior_title = False, prior_a_score = False):\n",
    "    \n",
    "    original_df = df_from_4_information_file(path_documents_ex_3)\n",
    "    df_cleaned = clean_df(original_df, \"animeDescription\")\n",
    "    df_score_desc = execute_similarity_query(query, k, df_cleaned, vocabulary,index , invert_index, original_df, path_documents_file, \"animeDescription\")\n",
    "    \n",
    "    if prior_title and prior_a_score:\n",
    "        df_cleaned_title = clean_df(original_df, \"animeTitle\")\n",
    "        df_score_title = execute_similarity_query(query, k, df_cleaned_title, vocabulary_title,index_title , invert_index_title, original_df, path_documents_file, \"animeTitle\")\n",
    "        # Merge the two dataframes to have all scores together\n",
    "        df_result = df_score_desc.reset_index().merge(df_score_title.reset_index(), how=\"outer\").set_index('index')\n",
    "        \n",
    "        # Check if all columns exist after merging\n",
    "        if 'Similarity animeDescription' not in df_result:\n",
    "            df_result['Similarity animeDescription'] = 0\n",
    "        if 'Similarity animeTitle' not in df_result:\n",
    "            df_result['Similarity animeTitle'] = 0\n",
    "            \n",
    "        # Fill NaN values with zeros\n",
    "        df_result = df_result.fillna(0)\n",
    "        df_result['FinalScore'] = (np.array(df_result['Similarity animeDescription'])+ 2*np.array(df_result['Similarity animeTitle'])+ (np.array(df_score_desc['animeScore'])/10))/4\n",
    "        df_result = df_result.sort_values(by=['FinalScore'], ascending=False)\n",
    "        return df_result.head(k)\n",
    "    \n",
    "    elif prior_title:\n",
    "        df_cleaned_title = clean_df(original_df, \"animeTitle\")\n",
    "        df_score_title = execute_similarity_query(query, k, df_cleaned_title, vocabulary_title,index_title , invert_index_title, original_df, path_documents_file, \"animeTitle\")\n",
    "        # Merge the two dataframes to have all scores together\n",
    "        df_result = df_score_desc.reset_index().merge(df_score_title.reset_index(), how=\"outer\").set_index('index')\n",
    "        \n",
    "        # Check if all columns exist after merging\n",
    "        if 'Similarity animeDescription' not in df_result:\n",
    "            df_result['Similarity animeDescription'] = 0\n",
    "        if 'Similarity animeTitle' not in df_result:\n",
    "            df_result['Similarity animeTitle'] = 0\n",
    "            \n",
    "        # Fill NaN values with zeros\n",
    "        df_result = df_result.fillna(0)\n",
    "        df_result['FinalScore'] = (np.array(df_result['Similarity animeDescription'])+ 2*np.array(df_result['Similarity animeTitle']))/3\n",
    "        df_result = df_result.sort_values(by=['FinalScore'], ascending=False)\n",
    "        return df_result.head(k)\n",
    "    \n",
    "    elif prior_a_score:\n",
    "        # Check if all columns exist\n",
    "        if 'Similarity animeDescription' not in df_result:\n",
    "            df_result['Similarity animeDescription'] = 0\n",
    "            \n",
    "        df_score_desc['FinalScore'] = (np.array(df_score_desc['Similarity animeDescription'])+ (np.array(df_score_desc['animeScore'])/10))/2\n",
    "        df_score_desc = df_score_desc.sort_values(by=['FinalScore'], ascending=False)\n",
    "        return df_score_desc.head(k)\n",
    "    \n",
    "    else:\n",
    "        df_score_desc = df_score_desc.sort_values(by=['Similarity animeDescription'], ascending=False)\n",
    "        return df_score_desc.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3c971637-a80e-48f6-b002-0490b58db3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>animeScore</th>\n",
       "      <th>Similarity animeDescription</th>\n",
       "      <th>FinalScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/28977/Gintama°\\r\\n</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.482984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Nichijou</td>\n",
       "      <td>Nichijou primarily focuses on the daily antics...</td>\n",
       "      <td>https://myanimelist.net/anime/31181/Owarimonog...</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.080656</td>\n",
       "      <td>0.463328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             animeTitle  \\\n",
       "index                                     \n",
       "0      Fullmetal Alchemist: Brotherhood   \n",
       "124                            Nichijou   \n",
       "\n",
       "                                        animeDescription  \\\n",
       "index                                                      \n",
       "0      After a horrific alchemy experiment goes wrong...   \n",
       "124    Nichijou primarily focuses on the daily antics...   \n",
       "\n",
       "                                                     Url  animeScore  \\\n",
       "index                                                                  \n",
       "0       https://myanimelist.net/anime/28977/Gintama°\\r\\n        9.16   \n",
       "124    https://myanimelist.net/anime/31181/Owarimonog...        8.46   \n",
       "\n",
       "       Similarity animeDescription  FinalScore  \n",
       "index                                           \n",
       "0                         0.049967    0.482984  \n",
       "124                       0.080656    0.463328  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.merge(result1, result2, how=\"outer\")\n",
    "result = result2.reset_index().merge(result1.reset_index(), how=\"outer\").set_index('index')\n",
    "result = result.fillna(0)\n",
    "#result[\"scorefinale\"] = (np.array(result['Similarity animeDescription'])+ 2*np.array(result['Similarity animeTitle']))/3\n",
    "result['FinalScore'] = (np.array(result['Similarity animeDescription'])+ (np.array(result['animeScore'])/10))/2\n",
    "result = result.sort_values(by=['FinalScore'], ascending=False)\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "53aae322-d83b-44f6-b89a-17f01cd85997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>animeScore</th>\n",
       "      <th>Similarity animeDescription</th>\n",
       "      <th>Similarity animeTitle</th>\n",
       "      <th>FinalScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Nichijou</td>\n",
       "      <td>Nichijou primarily focuses on the daily antics...</td>\n",
       "      <td>https://myanimelist.net/anime/31181/Owarimonog...</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.080656</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>The Abyss—a gaping chasm stretching down into ...</td>\n",
       "      <td>https://myanimelist.net/anime/2921/Ashita_no_J...</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Tengen Toppa Gurren Lagann</td>\n",
       "      <td>Simon and Kamina were born and raised in a dee...</td>\n",
       "      <td>https://myanimelist.net/anime/34591/Natsume_Yu...</td>\n",
       "      <td>8.64</td>\n",
       "      <td>0.050121</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Neon Genesis Evangelion: The End of Evangelion</td>\n",
       "      <td>Shinji Ikari is left emotionally comatose afte...</td>\n",
       "      <td>https://myanimelist.net/anime/38474/Yuru_Camp△...</td>\n",
       "      <td>8.54</td>\n",
       "      <td>0.050071</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>https://myanimelist.net/anime/28977/Gintama°\\r\\n</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           animeTitle  \\\n",
       "index                                                   \n",
       "124                                          Nichijou   \n",
       "45                                      Made in Abyss   \n",
       "62                         Tengen Toppa Gurren Lagann   \n",
       "90     Neon Genesis Evangelion: The End of Evangelion   \n",
       "0                    Fullmetal Alchemist: Brotherhood   \n",
       "\n",
       "                                        animeDescription  \\\n",
       "index                                                      \n",
       "124    Nichijou primarily focuses on the daily antics...   \n",
       "45     The Abyss—a gaping chasm stretching down into ...   \n",
       "62     Simon and Kamina were born and raised in a dee...   \n",
       "90     Shinji Ikari is left emotionally comatose afte...   \n",
       "0      After a horrific alchemy experiment goes wrong...   \n",
       "\n",
       "                                                     Url  animeScore  \\\n",
       "index                                                                  \n",
       "124    https://myanimelist.net/anime/31181/Owarimonog...        8.46   \n",
       "45     https://myanimelist.net/anime/2921/Ashita_no_J...        8.70   \n",
       "62     https://myanimelist.net/anime/34591/Natsume_Yu...        8.64   \n",
       "90     https://myanimelist.net/anime/38474/Yuru_Camp△...        8.54   \n",
       "0       https://myanimelist.net/anime/28977/Gintama°\\r\\n        9.16   \n",
       "\n",
       "       Similarity animeDescription  Similarity animeTitle  FinalScore  \n",
       "index                                                                  \n",
       "124                       0.080656                      0    0.026885  \n",
       "45                        0.051465                      0    0.017155  \n",
       "62                        0.050121                      0    0.016707  \n",
       "90                        0.050071                      0    0.016690  \n",
       "0                         0.049967                      0    0.016656  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"robot\"\n",
    "k = 5\n",
    "\n",
    "original_df = df_from_4_information_file(path_documents_ex_3)\n",
    "\n",
    "vocabulary = read_vocabulary(path_vocabulary_file)\n",
    "index = read_index(path_index_file)\n",
    "write_inverted_index(original_df, vocabulary, index, \"animeDescription\", path_inverted_index_file)\n",
    "invert_index = read_inverted_index(path_inverted_index_file)\n",
    "\n",
    "vocabulary_title = read_vocabulary(path_vocabulary_title_file)\n",
    "index_title = read_index(path_index_title_file)\n",
    "write_inverted_index(original_df, vocabulary_title, index_title, \"animeTitle\", path_inverted_index_title_file)\n",
    "invert_index_title = read_inverted_index(path_inverted_index_title_file)\n",
    "\n",
    "result = execute_new_score_query(query, k, vocabulary, index, invert_index, vocabulary_title, index_title, invert_index_title, \n",
    "                            path_documents_file, prior_title = True, prior_a_score = False)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2Aris",
   "language": "python",
   "name": "hw2aris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
